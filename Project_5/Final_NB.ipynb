{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import my_tools \n",
    "\n",
    "\n",
    "### define constants for base feature names\n",
    "FINANCIAL_FEATURES = [\"bonus\", \"deferral_payments\", \"deferred_income\", \"director_fees\",\n",
    "                      \"exercised_stock_options\", \"expenses\", \"loan_advances\",\n",
    "                      \"long_term_incentive\", \"other\", \"restricted_stock\", \n",
    "                      \"restricted_stock_deferred\", \"salary\", \"total_payments\",\n",
    "                      \"total_stock_value\"]\n",
    "\n",
    "EMAIL_FEATURES = [\"from_messages\", \"to_messages\"]\n",
    "EMAIL_POI_FEATURES = [\"from_poi_to_this_person\", \"from_this_person_to_poi\", \"shared_receipt_with_poi\"]\n",
    "\n",
    "## These financial features have data point in at least 50% of rows\n",
    "FINANCIAL_FEATURES_50PCT = [\"bonus\", \"exercised_stock_options\", \"expenses\",\n",
    "                      \"other\", \"restricted_stock\", \"salary\", \"total_payments\",\n",
    "                      \"total_stock_value\"]\n",
    "\n",
    "### These are the features that appear on visual inspection to have normal-shaped distribution after\n",
    "### log-transform\n",
    "FINANCIAL_FEATURES_LOG_NL_SHAPE = [\"bonus\", \"exercised_stock_options\", \"restricted_stock\", \n",
    "                       \"salary\", \"total payments\", \"total_stock_value\"  ]\n",
    "\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "### 2 entries are not individuals. one labeled \"TOTAL\" and the other\n",
    "### \"THE TRAVEL AGENCY IN THE PARK\"\n",
    "del data_dict[\"TOTAL\"]\n",
    "del data_dict[\"THE TRAVEL AGENCY IN THE PARK\"]\n",
    "\n",
    "### Unclear whether other outliers should be removed\n",
    "\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exercised_stock_options :\t24.8150797332\n",
      "total_stock_value :\t24.1828986786\n",
      "bonus :\t20.7922520472\n",
      "salary :\t18.2896840434\n",
      "deferred_income :\t11.4584765793\n",
      "long_term_incentive :\t9.92218601319\n",
      "restricted_stock :\t9.21281062198\n",
      "total_payments :\t8.77277773009\n",
      "shared_receipt_with_poi :\t8.58942073168\n",
      "loan_advances :\t7.18405565829\n",
      "expenses :\t6.09417331064\n",
      "from_poi_to_this_person :\t5.24344971337\n",
      "other :\t4.187477507\n",
      "from_this_person_to_poi :\t2.38261210823\n",
      "director_fees :\t2.12632780201\n",
      "to_messages :\t1.64634112944\n",
      "deferral_payments :\t0.224611274736\n",
      "from_messages :\t0.169700947622\n",
      "restricted_stock_deferred :\t0.0654996529099\n"
     ]
    }
   ],
   "source": [
    "### this is a list of all the base features supplied by Udacity\n",
    "base_features_list = ['poi'] + FINANCIAL_FEATURES + EMAIL_FEATURES + EMAIL_POI_FEATURES\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, base_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### In this step, fit a SKB selector to all of the feqtures and print out the feature names in order of\n",
    "### importance.  Note: this is fitting to ALL of the data,\n",
    "### which really isn't kosher, since one would only have a training set for any fitting step in a real\n",
    "### application.  \n",
    "skb = SelectKBest(score_func = f_classif, k=len(base_features_list)-1)\n",
    "skb.fit(features, labels)\n",
    "### create a new sorted feature list\n",
    "skb_features_list = []\n",
    "feature_scores = dict(zip(base_features_list[1:], skb.scores_))\n",
    "for key in sorted(feature_scores, key=feature_scores.get, reverse = True):\n",
    "    print key,\":\\t\", feature_scores[key]\n",
    "    skb_features_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: Pipeline(steps=[('skb', SelectKBest(k=5, score_func=<function f_classif at 0x00000000072CB978>)), ('gnb', GaussianNB())])\n",
      "F1 score: 0.398571428571\n"
     ]
    }
   ],
   "source": [
    "### Use a pipeline and gridsearchCV to pick best number of features from base features list. F1 score as\n",
    "### evaluation metric for scoring the combination.\n",
    "skb = SelectKBest(score_func = f_classif)\n",
    "gnb = GaussianNB()\n",
    "pl = Pipeline([('skb', skb), ('gnb', gnb)])\n",
    "### Use SSS cross-validator. To save time while testing, n_iter is 100, not 1000 as\n",
    "### in the Udacity tester.py code.\n",
    "cv = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "param_grid = {'skb__k' : [x for x in range(1, len(base_features_list))]}\n",
    "gs = GridSearchCV(pl, param_grid, scoring = 'f1', cv = cv)\n",
    "gs.fit(features, labels)\n",
    "print \"Best Estimator:\", gs.best_estimator_\n",
    "print \"F1 score:\", gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline SelectKBest(k=5) -> Gaussian(NB), using base_features_list features.\n",
      "Accuracy \t Precision \t Recall \t F1 \t\t F2\n",
      "0.849 \t\t 0.422 \t\t 0.359 \t\t 0.388 \t\t 0.370\n"
     ]
    }
   ],
   "source": [
    "### Pass this combination to the tester.py algorithm and print results\n",
    "skb = SelectKBest(k = 5, score_func = f_classif)\n",
    "gnb = GaussianNB()\n",
    "pl = Pipeline([('skb', skb), ('gnb', gnb)])\n",
    "print \"Testing pipeline SelectKBest(k=5) -> Gaussian(NB), using base_features_list features.\"\n",
    "accuracy, precision, recall, f1, f2 = my_tools.my_test_classifier(pl, my_dataset, base_features_list, folds = 1000)\n",
    "print \"Accuracy \\t Precision \\t Recall \\t F1 \\t\\t F2\"\n",
    "print \"%1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f\" %(accuracy, precision, recall, f1, f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and precision are already over the requirement for the project.  At this point we know what best number of features is, but not which ones actually used.  This can be different on each pass through the test algorithm, as the training features against which SelectKBest runs will be different each time.  Count how many times each feature is chosen and print sorted list of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_stock_value :\t1000\n",
      "exercised_stock_options :\t998\n",
      "bonus :\t997\n",
      "salary :\t989\n",
      "deferred_income :\t506\n",
      "long_term_incentive :\t190\n",
      "restricted_stock :\t133\n",
      "shared_receipt_with_poi :\t109\n",
      "total_payments :\t58\n",
      "expenses :\t13\n",
      "from_poi_to_this_person :\t7\n",
      "to_messages :\t0\n",
      "deferral_payments :\t0\n",
      "loan_advances :\t0\n",
      "restricted_stock_deferred :\t0\n",
      "from_messages :\t0\n",
      "other :\t0\n",
      "from_this_person_to_poi :\t0\n",
      "director_fees :\t0\n"
     ]
    }
   ],
   "source": [
    "### Set up features\n",
    "data = featureFormat(my_dataset, base_features_list, sort_keys = True )\n",
    "labels, features = targetFeatureSplit(data)\n",
    "### get arrays of train/test indices from StratifiedShuffleSplit\n",
    "cv = StratifiedShuffleSplit(labels, n_iter = 1000, random_state = 42)\n",
    "### save the feature counts in this dictionary\n",
    "features_use_count = np.zeros(len(base_features_list)-1, dtype = int)\n",
    "### get the indices from each iteration and create the training matrices\n",
    "for train_indices, test_indices in cv:\n",
    "    labels_train = []\n",
    "    features_train = []\n",
    "    for train_ix in train_indices:\n",
    "        labels_train.append(labels[train_ix])\n",
    "        features_train.append(features[train_ix])\n",
    "    skb = SelectKBest(score_func = f_classif, k = 5)\n",
    "    skb.fit(features_train, labels_train)\n",
    "    for ix in skb.get_support(indices = True):\n",
    "        features_use_count[ix] += 1\n",
    "\n",
    "feature_scores = dict(zip(base_features_list[1:],features_use_count))\n",
    "for key in sorted(feature_scores, key=feature_scores.get, reverse = True):\n",
    "    print key,\":\\t\", feature_scores[key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that an email feature, shared_receipt_with_poi, is a little higher in this list than in the feature rankings done by SelectKBest against all of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GaussianNB with features: ['total_stock_value', 'bonus', 'exercised_stock_options', 'salary', 'deferred_income']\n",
      "Accuracy \t Precision \t Recall \t F1 \t\t F2\n",
      "0.855 \t\t 0.489 \t\t 0.381 \t\t 0.428 \t\t 0.398\n"
     ]
    }
   ],
   "source": [
    "### Create a features list with just the top five in terms of use count, and feed that through the tester again.\n",
    "test_features_list = ['poi', 'total_stock_value', 'bonus', 'exercised_stock_options', 'salary', 'deferred_income']\n",
    "print \"Testing GaussianNB with features:\", test_features_list[1:]\n",
    "clf = GaussianNB()\n",
    "accuracy, precision, recall, f1, f2 = my_tools.my_test_classifier(clf, my_dataset, test_features_list, folds = 1000)\n",
    "print \"Accuracy \\t Precision \\t Recall \\t F1 \\t\\t F2\"\n",
    "print \"%1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f\" %(accuracy, precision, recall, f1, f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is slightly higher than the performance of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing modified feature list using log-transforms of bonus, exercised_stock_options,\n",
      "salary, and total_stock_value.\n",
      "accuracy \t precision \t recall \t f1 \t\t f2\n",
      "0.859 \t\t 0.511 \t\t 0.225 \t\t 0.312 \t\t 0.253\n"
     ]
    }
   ],
   "source": [
    "### Of top 5 in this list, 4 have more normal appearing shape with log transform  Apply this and try\n",
    "### these features to see if any improvement.  \n",
    "\n",
    "### Create new features for each of the financial features whose distribution has a more normal shaped\n",
    "### histogram with a log transform. Substitute this for the base features and see what performance is like.\n",
    "\n",
    "LOG_NORMAL_FEATURES = []\n",
    "for feature in FINANCIAL_FEATURES_LOG_NL_SHAPE:\n",
    "    my_tools.create_log_feature(my_dataset, feature, abs_value = True) \n",
    "    LOG_NORMAL_FEATURES.append(\"log_\" + feature)\n",
    "\n",
    "test_features_list = ['poi'] + ['log_bonus', 'deferred_income', 'log_exercised_stock_options', \n",
    "                           'log_salary', 'log_total_stock_value']\n",
    "print \"Testing modified feature list using log-transforms of bonus, exercised_stock_options,\"\n",
    "print \"salary, and total_stock_value.\" \n",
    "print \"accuracy \\t precision \\t recall \\t f1 \\t\\t f2\"\n",
    "clf = GaussianNB()\n",
    "accuracy, precision, recall, f1, f2 = my_tools.my_test_classifier(clf, my_dataset, test_features_list, folds = 1000)\n",
    "print \"%1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f\" %(accuracy, precision, recall, f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: Pipeline(steps=[('skb', SelectKBest(k=19, score_func=<function f_classif at 0x00000000072CB978>)), ('gnb', GaussianNB())])\n",
      "F1 score: 0.278939282939\n"
     ]
    }
   ],
   "source": [
    "### So that didn't seem to help.  Substitute  all the log-transformed features back into the SelectKBest\n",
    "### routine and see if there is any improvement.\n",
    "\n",
    "test_features_list = ['poi'] + [\"log_bonus\", \"deferral_payments\", \"deferred_income\", \n",
    "                           \"director_fees\", \"log_exercised_stock_options\", \"expenses\", \n",
    "                           \"loan_advances\", \"long_term_incentive\", \"other\", \"restricted_stock\", \n",
    "                           \"restricted_stock_deferred\", \"log_salary\", \n",
    "                           \"total_payments\", \"log_total_stock_value\"] + \\\n",
    "                            EMAIL_FEATURES + EMAIL_POI_FEATURES\n",
    "\n",
    "### Extract features and labels from dataset again for testing\n",
    "data = featureFormat(my_dataset, test_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "skb = SelectKBest(score_func = f_classif)\n",
    "gnb = GaussianNB()\n",
    "pl = Pipeline([('skb', skb), ('gnb', gnb)])\n",
    "### Use SSS cross-validator. For purposes of testing, this is same as in the Udacity tester.py, except\n",
    "### n_iter is 100 instead of 1000, for the purpose of saving time.  \n",
    "cv = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "param_grid = {'skb__k' : [x for x in range(1, len(base_features_list))]}\n",
    "gs = GridSearchCV(pl, param_grid, scoring = 'f1', cv = cv)\n",
    "gs.fit(features, labels)\n",
    "print \"Best Estimator:\", gs.best_estimator_\n",
    "print \"F1 score:\", gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So adding specific log-transformed features, or substituting all didn't improve the performance of a Gaussian Naive Bayes classifier.  Now use the email feature which was the highest in the ranking by feature count: shared_receipt_with_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GaussianNB with features: ['bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value', 'shared_receipt_with_poi']\n",
      "accuracy \t precision \t recall \t f1 \t\t f2\n",
      " 0.852 \t\t 0.474 \t\t 0.363 \t\t 0.411 \t\t 0.381\n"
     ]
    }
   ],
   "source": [
    "### Testing adding highest-ranked email feature to list of top 5 financial features\n",
    "test_features_list = ['poi'] + ['bonus', 'deferred_income', 'exercised_stock_options', \n",
    "                           'salary', 'total_stock_value'] + [\"shared_receipt_with_poi\"]\n",
    "\n",
    "print \"Testing GaussianNB with features:\", test_features_list[1:]\n",
    "print \"accuracy \\t precision \\t recall \\t f1 \\t\\t f2\"\n",
    "clf = GaussianNB()\n",
    "accuracy, precision, recall, f1, f2 = my_tools.my_test_classifier(clf, my_dataset, test_features_list, folds = 1000)\n",
    "print \" %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f\" %(accuracy, precision, recall, f1, f2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't score as well as just the top 5 financial features alone.  Try adding a ratio feature which normalizes this by dividing it by \"to_messages\", presumably the total messages received by any one individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GaussianNB with features: ['bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value', 'shared_receipt_ratio']\n",
      "accuracy \t precision \t recall \t f1 \t\t f2\n",
      "0.861 \t\t 0.516 \t\t 0.386 \t\t 0.441 \t\t 0.406\n"
     ]
    }
   ],
   "source": [
    "### create the new feature, add to dataset, run the cross-validation scorer\n",
    "my_tools.create_ratio_feature(my_dataset, \"shared_receipt_ratio\", \"shared_receipt_with_poi\", \"to_messages\")\n",
    "test_features_list = ['poi'] + ['bonus', 'deferred_income', 'exercised_stock_options', \n",
    "                           'salary', 'total_stock_value'] + [\"shared_receipt_ratio\"]\n",
    "clf = GaussianNB()\n",
    "print \"Testing GaussianNB with features:\", test_features_list[1:]\n",
    "print \"accuracy \\t precision \\t recall \\t f1 \\t\\t f2\"\n",
    "accuracy, precision, recall, f1, f2 = my_tools.my_test_classifier(clf, my_dataset, test_features_list, folds = 1000)\n",
    "print \"%1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f\" %(accuracy, precision, recall, f1, f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is better than the previous best of:\n",
    "\n",
    "Accuracy \t Precision \t Recall \t F1 \t\t F2\n",
    "\n",
    "0.855 \t\t 0.489 \t\t 0.381 \t\t 0.428 \t\t 0.398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GaussianNB with feature list: ['bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value', 'poi_email_ratio']\n",
      "0.861 \t\t 0.516 \t\t 0.386 \t\t 0.441 \t\t 0.406\n"
     ]
    }
   ],
   "source": [
    "### Try making another ratio feature: <total poi-related messages>/<all messages>\n",
    "my_tools.create_ratio_feature(my_dataset, \"poi_email_ratio\", EMAIL_POI_FEATURES, EMAIL_FEATURES)\n",
    "test_features_list = ['poi'] + ['bonus', 'deferred_income', 'exercised_stock_options', \n",
    "                           'salary', 'total_stock_value'] + [\"poi_email_ratio\"]\n",
    "clf = GaussianNB()\n",
    "print \"Testing GaussianNB with feature list:\", test_features_list[1:]\n",
    "accuracy, precision, recall, f1, f2 = my_tools.my_test_classifier(clf, my_dataset, test_features_list, folds = 1000)\n",
    "print \"%1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f\" %(accuracy, precision, recall, f1, f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So adding either email feature improves overall performance the same amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features \t accuracy \t precision \t recall \t f1 \t\t f2\n",
      " 1 \t\t 0.807 \t\t 0.013 \t\t 0.002 \t\t 0.003 \t\t 0.002\n",
      " 2 \t\t 0.810 \t\t 0.327 \t\t 0.133 \t\t 0.189 \t\t 0.150\n",
      " 3 \t\t 0.828 \t\t 0.341 \t\t 0.123 \t\t 0.181 \t\t 0.142\n",
      " 4 \t\t 0.820 \t\t 0.236 \t\t 0.116 \t\t 0.156 \t\t 0.129\n",
      " 5 \t\t 0.856 \t\t 0.433 \t\t 0.246 \t\t 0.314 \t\t 0.269\n",
      " 6 \t\t 0.856 \t\t 0.436 \t\t 0.273 \t\t 0.336 \t\t 0.295\n",
      " 7 \t\t 0.851 \t\t 0.398 \t\t 0.231 \t\t 0.292 \t\t 0.252\n",
      " 8 \t\t 0.847 \t\t 0.371 \t\t 0.211 \t\t 0.269 \t\t 0.231\n"
     ]
    }
   ],
   "source": [
    "### That's the same as the first email ratio feature.\n",
    "### One more approach to feature selection: do it by hand.  Select financial features which have the highest\n",
    "### ratio of the median(poi)/median(non-poi) in the zero-substituted dataset.  Test increasing number of those\n",
    "### features, but always include the \"shared_receipt_ratio\" feature.\n",
    "HIGH_MEDIAN_RATIO_FEATURES = [\"other\", \"bonus\", \"expenses\", \"restricted_stock\", \"total_stock_value\",\n",
    "                             \"exercised_stock_options\", \"total_payments\", \"salary\"]\n",
    "gnb = GaussianNB()\n",
    "i = 0\n",
    "test_features_list = ['poi', 'shared_receipt_ratio']\n",
    "print \"n_features \\t accuracy \\t precision \\t recall \\t f1 \\t\\t f2\"\n",
    "for feature in HIGH_MEDIAN_RATIO_FEATURES:\n",
    "    test_features_list = test_features_list + [feature]\n",
    "    i += 1\n",
    "    accuracy, precision, recall, f1, f2 = my_tools.my_test_classifier(gnb, my_dataset, test_features_list, folds = 1000)\n",
    "    print \" %d \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f\" %(i, accuracy, precision, recall, f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=7, whiten=False)), ('gnb', GaussianNB())])\n",
      "F1 score for this estimator: 0.387666666667\n"
     ]
    }
   ],
   "source": [
    "### So that approach doesn't achieve goal.  Now since NB assumes feature independence,\n",
    "### and some of the financial features are highly correlated, run a PCA to make features as orthogonal\n",
    "### as possible.  Do grid search on FINANCIAL_FEATURES first\n",
    "\n",
    "### reload the features\n",
    "test_features_list = ['poi'] + FINANCIAL_FEATURES\n",
    "data = featureFormat(my_dataset, test_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "pca = PCA()\n",
    "gnb = GaussianNB()\n",
    "cv = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "pl = Pipeline([('scaler', mms),('pca', pca), ('gnb', gnb) ])\n",
    "param_grid = {'pca__n_components' : [x for x in range(1,len(test_features_list))]}\n",
    "gs = GridSearchCV(pl, param_grid, scoring = 'f1', cv = cv)\n",
    "gs.fit(features, labels)\n",
    "print \"Best Estimator:\", gs.best_estimator_\n",
    "print \"F1 score for this estimator:\", gs.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=11, whiten=False)), ('gnb', GaussianNB())])\n",
      "F1 score for this estimator: 0.396619047619\n"
     ]
    }
   ],
   "source": [
    "### reload the features, now use all base features\n",
    "test_features_list = base_features_list\n",
    "data = featureFormat(my_dataset, test_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "pca = PCA()\n",
    "gnb = GaussianNB()\n",
    "cv = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "pl = Pipeline([('scaler', mms),('pca', pca), ('gnb', gnb) ])\n",
    "param_grid = {'pca__n_components' : [x for x in range(1,len(test_features_list))]}\n",
    "gs = GridSearchCV(pl, param_grid, scoring = 'f1', cv = cv)\n",
    "gs.fit(features, labels)\n",
    "print \"Best Estimator:\", gs.best_estimator_\n",
    "print \"F1 score for this estimator:\", gs.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=2, whiten=False)), ('gnb', GaussianNB())])\n",
      "F1 score for this estimator: 0.486666666667\n"
     ]
    }
   ],
   "source": [
    "### reload the features, use top five financial features\n",
    "test_features_list = ['poi', 'total_stock_value', 'bonus', 'exercised_stock_options', 'salary', 'deferred_income']\n",
    "data = featureFormat(my_dataset, test_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "pca = PCA()\n",
    "gnb = GaussianNB()\n",
    "cv = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "pl = Pipeline([('scaler', mms),('pca', pca), ('gnb', gnb) ])\n",
    "param_grid = {'pca__n_components' : [x for x in range(1,len(test_features_list))]}\n",
    "gs = GridSearchCV(pl, param_grid, scoring = 'f1', cv = cv)\n",
    "gs.fit(features, labels)\n",
    "print \"Best Estimator:\", gs.best_estimator_\n",
    "print \"F1 score for this estimator:\", gs.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=4, whiten=False)), ('gnb', GaussianNB())])\n",
      "F1 score for this estimator: 0.460333333333\n"
     ]
    }
   ],
   "source": [
    "### reload the features, use top five financial features + shared_receipt_ratio\n",
    "test_features_list = ['poi', 'total_stock_value', 'bonus', 'exercised_stock_options', 'salary', 'deferred_income',\n",
    "                     \"shared_receipt_ratio\"]\n",
    "data = featureFormat(my_dataset, test_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "pca = PCA()\n",
    "gnb = GaussianNB()\n",
    "cv = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "pl = Pipeline([('scaler', mms),('pca', pca), ('gnb', gnb) ])\n",
    "param_grid = {'pca__n_components' : [x for x in range(1,len(test_features_list))]}\n",
    "gs = GridSearchCV(pl, param_grid, scoring = 'f1', cv = cv)\n",
    "gs.fit(features, labels)\n",
    "print \"Best Estimator:\", gs.best_estimator_\n",
    "print \"F1 score for this estimator:\", gs.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline MinMaxScaler -> PCA(n_components = 2) -> GaussianNB, with feature list:\n",
      "['poi', 'total_stock_value', 'bonus', 'exercised_stock_options', 'salary', 'deferred_income']\n",
      "accuracy \t precision \t recall \t f1 \t\t f2\n",
      "0.877 \t\t 0.616 \t\t 0.371 \t\t 0.463 \t\t 0.403\n"
     ]
    }
   ],
   "source": [
    "### Run the best estimator from above through full testing\n",
    "### reload the features, use top five financial features\n",
    "test_features_list = ['poi', 'total_stock_value', 'bonus', 'exercised_stock_options', 'salary', 'deferred_income']\n",
    "data = featureFormat(my_dataset, test_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "pca = PCA(n_components = 2)\n",
    "gnb = GaussianNB()\n",
    "pl = Pipeline([('scaler', mms),('pca', pca), ('gnb', gnb) ])\n",
    "print \"Testing pipeline MinMaxScaler -> PCA(n_components = 2) -> GaussianNB, with feature list:\"\n",
    "print test_features_list\n",
    "print \"accuracy \\t precision \\t recall \\t f1 \\t\\t f2\"\n",
    "accuracy, precision, recall, f1, f2 = my_tools.my_test_classifier(pl, my_dataset, test_features_list, folds = 1000)\n",
    "print \"%1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f\" %(accuracy, precision, recall, f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GaussianNB, custom imputation, with features: ['bonus', 'deferred_income', 'exercised_stock_options', 'salary', 'total_stock_value', 'shared_receipt_ratio']\n",
      "accuracy \t precision \t recall \t f1 \t\t f2\n",
      "0.865 \t\t 0.490 \t\t 0.364 \t\t 0.418 \t\t 0.384\n"
     ]
    }
   ],
   "source": [
    "### Reload dataset, create custom features, run custom imputer on salary feature, test classifier\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "### 2 entries are not individuals. one labeled \"TOTAL\" and the other\n",
    "### \"THE TRAVEL AGENCY IN THE PARK\"\n",
    "del data_dict[\"TOTAL\"]\n",
    "del data_dict[\"THE TRAVEL AGENCY IN THE PARK\"]\n",
    "\n",
    "### Unclear whether other outliers should be removed\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "my_tools.my_imputer(my_dataset, \"salary\", strategy = 'median', test = my_tools.is_not_director)\n",
    "\n",
    "my_tools.create_ratio_feature(my_dataset, \"shared_receipt_ratio\", \"shared_receipt_with_poi\", \"to_messages\")\n",
    "\n",
    "test_features_list = ['poi'] + ['bonus', 'deferred_income', 'exercised_stock_options', \n",
    "                           'salary', 'total_stock_value'] + [\"shared_receipt_ratio\"]\n",
    "clf = GaussianNB()\n",
    "print \"Testing GaussianNB, custom imputation, with features:\", test_features_list[1:]\n",
    "print \"accuracy \\t precision \\t recall \\t f1 \\t\\t f2\"\n",
    "accuracy, precision, recall, f1, f2 = my_tools.my_test_classifier(clf, my_dataset, test_features_list, folds = 1000)\n",
    "print \"%1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f \\t\\t %1.3f\" %(accuracy, precision, recall, f1, f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this doesn't improve performance, although it still meets goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
